# -*- coding: utf-8 -*-
"""autos_project_nbook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/126ks82_hIGs9CaC_PuYbJuaixHNaRvA8
"""

# Commented out IPython magic to ensure Python compatibility.
'''At first, we're going to import necessary libraries.'''

import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import seaborn as sns
from pandas_profiling import ProfileReport
import time
import math
import warnings
warnings.filterwarnings("ignore")

'''The second action is a repository cloning from GitHub and dataset loading.'''

!git clone https://github.com/slatkowski/autos_project
path_to_file = 'autos_project/autos.csv'
df = pd.read_csv(path_to_file)
df

'''Next we should count all values - function to use in this purpose is defined below.'''

def DFCounter(df):
    for col in df.columns:
        print(df[col].value_counts())
        print('_______________________')
        
DFCounter(df)

'''"Andere" means "other" in German.
These values can hide any other value from adequate column,
so replacing them with NaN would be a correct action.'''

df.replace('andere', np.nan, inplace=True)

'''Data used in this project comes from url https://data.world/data-society/used-cars-data.
It has been scraped from eBay Kleinanzeigen and refers to car selling advertisement.
We have to check when the first and the last advertisement
have been published to make data filtering correct.'''

print(f"Date of the first advertisement: {df['dateCrawled'].min()}.")
print(f"Date of the last advertisement: {df['dateCrawled'].max()}.")

'''Column "index" contains unique values from 0 to 371528, so we can set it as index column.'''

df.set_index('index', inplace=True)

'''First information about a DataFrame columns names, non-null values and data types.
Columns "vehicleType", "gearbox", "model", "fuelType" and "notRepairedDamage" contain NaN values.'''

df.info()

'''Next, take a look at the descriptive statistics of DataFrame
(values round to two places after a comma to better readability).'''

df.describe().apply(lambda x: x.apply('{0:.2f}'.format))

'''To replace NaNs with values we should define a function
which replaces them with values according to frequency of their appearance
in the column where NaNs appear.'''

from collections import Counter
import random
def NANFiller(df):
    def columnFiller(series):
        nan_c = len(series[series.isna()])
        nnan_c = series[series.notna()]
        count_nn = Counter(nnan_c)    
        new_val = random.choices(list(count_nn.keys()), weights = list(count_nn.values()), k=nan_c)
        series[series.isna()] = new_val
        return series
    for col in df.columns:
        df[col]=columnFiller(df[col])
        
NANFiller(df)

'''The next description of DataFrame - as we can see, NaNs have been replaced.'''

df.info()

df = df.loc[(df.brand == 'volkswagen') | (df.brand == 'bmw') | (df.brand == 'mercedes_benz') | (df.brand == 'opel') | (df.brand == 'audi')]

df = df[(df['yearOfRegistration'] >= 1976) & (df['yearOfRegistration'] <= 2016)]
df = df[(df['price'] >= 250) & (df['price'] <= 60000)]
df = df[(df['powerPS'] >= 25) & (df['powerPS'] <= 600)]

df.info()

bins = [0, 16999, 28999, 39999, 69999, 89999]

df['postalCode'] = pd.cut(df['postalCode'], bins=bins,
       labels=['Eastern', 'Northern', 'Central', 'Western', 'Southern'])

df['postalCode']

df['postalCode'].fillna('Central', inplace=True)
df['postalCode']

DFCounter(df)

df.info()

df.info()

df['brand'].value_counts()

min_cnt = df['brand'].value_counts().min()
df = df.groupby('brand').sample(min_cnt)

df['brand'].value_counts()

df.drop(columns=['dateCrawled', 'name', 'seller', 'offerType', 'abtest', 'model', 'monthOfRegistration',
       'dateCreated', 'nrOfPictures', 'lastSeen'], inplace=True)

df.columns

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

encode_list = ['gearbox', 'vehicleType', 'notRepairedDamage', 'fuelType', 'postalCode']

def EncodingDesc(series, le):
  transformed = le.fit_transform(series)
  print(series.name)
  print(le.classes_)
  print(np.unique(transformed))
  return transformed

for i in encode_list:
    EncodingDesc(df[i], le)
    print('______________________')

df.describe().apply(lambda x: x.apply('{0:.2f}'.format))

df.info()

sns.set(style="whitegrid") 
fig, ax = plt.subplots(figsize=(16,12))

for i, col in enumerate(df.columns):
    plt.figure(i)
    sns.countplot(x=col, data=df, hue='brand')

plt.show()

report = ProfileReport(df, infer_dtypes=False)
report

pivot = pd.pivot_table(df, index='brand', values = ['price', 'vehicleType', 'yearOfRegistration',
                                                      'gearbox', 'powerPS', 'fuelType',
                                                      'notRepairedDamage', 'kilometer', 'postalCode'], 
                       aggfunc= [np.mean, np.median, np.std, min, max])
pd.options.display.max_columns = None
display(pivot)

X = df.drop(columns='brand')
y = df['brand']

X.sample(5)

y.sample(5)

y = le.fit_transform(y)
y

from sklearn.feature_selection import mutual_info_classif
 
importances = mutual_info_classif(X, y)
 
feature_info = pd.Series(importances, X.columns).sort_values(ascending=False)
print(f'Five features providing the biggest information gain are:\n{feature_info.head(5)}')
print(f'Their cumulative information gain equals {np.around(np.sum(feature_info.head(5)), 2)}.')

from sklearn.feature_selection import SelectFromModel

def SelectorChoiceDisplay(estimator, X, y):
    selector = SelectFromModel(estimator=estimator).fit(X, y)
    print(selector.threshold_)
    print(X.columns)
    print(selector.get_support())
    print(selector.transform(X))

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()
SelectorChoiceDisplay(rfc, X, y)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size = 0.2,
                                                    random_state = 42,
                                                    stratify=y)

X_train, X_val, y_train, y_val = train_test_split(X_train,
                                                    y_train,
                                                    test_size = 0.25,
                                                    random_state = 42,
                                                    stratify=y_train)

print(f'X_train shape: {X_train.shape}')
print(f'X_val shape: {X_val.shape}')
print(f'X_test shape: {X_test.shape}')
print(f'y_train shape: {y_train.shape}')
print(f'y_val shape: {y_val.shape}')
print(f'y_test shape: {y_test.shape}')

first_model = RandomForestClassifier()

start = time.time()
first_model.fit(X_train, y_train)
first_model_train_pred = first_model.predict(X_train)
stop = time.time()
mins = math.floor((stop - start)/60)
seconds = math.ceil((stop - start)%60)
print(f"Training time of RandomForestClassifier with default params: {mins}:{seconds} mins.")

from sklearn.metrics import classification_report

first_model_val_pred = first_model.predict(X_val)
print(f'1st model - training metrics:\n\n{classification_report(y_train, first_model_train_pred)}')
print('_____________________________________________________\n')
print(f'1st model - validation metrics:\n\n{classification_report(y_val, first_model_val_pred)}')

from sklearn.model_selection import GridSearchCV

rfc = RandomForestClassifier()
params = {
    'max_depth': [10,15],
    'min_samples_split': [4,8],
    'min_samples_leaf': [2,4],
    'n_estimators': [150,200]
}
              
second_model = GridSearchCV(rfc, param_grid=params, 
                     cv=5, n_jobs=-1, verbose=4)

start = time.time()
second_model.fit(X_train, y_train)
second_model_train_pred = second_model.predict(X_train)
stop = time.time()
mins = math.floor((stop - start)/60)
seconds = math.ceil((stop - start)%60)
print(f"Training time of RandomForestClassifier with params chosen by GridSearchCV: {mins}:{seconds} mins.")

print(f'Params chosen by GridSearchCV for 2nd model: {second_model.best_estimator_}')
print(f'Best score gained by model with these params: {np.around(second_model.best_score_, 2)}')

second_model_val_pred = second_model.predict(X_val)
print(f'2nd model - training metrics:\n\n{classification_report(y_train, second_model_train_pred)}')
print('_____________________________________________________\n')
print(f'2nd model - validation metrics:\n\n{classification_report(y_val, second_model_val_pred)}')

rfc = RandomForestClassifier()

params = {
    'max_depth': [15, 20],
    'min_samples_split': [2, 4],
    'min_samples_leaf': [1, 2],
    'n_estimators': [250, 500, 1000]
}
              
third_model = GridSearchCV(rfc, param_grid=params, 
                     cv=5, n_jobs=-1, verbose=3)

start = time.time()
third_model.fit(X_train, y_train)
third_model_train_pred = third_model.predict(X_train)
stop = time.time()
mins = math.floor((stop - start)/60)
seconds = math.ceil((stop - start)%60)
print(f"Training time of RandomForestClassifier with params chosen by GridSearchCV: {mins}:{seconds} mins.")

print(f'Params chosen by GridSearchCV for 3rd model: {third_model.best_estimator_}')
print(f'Best score gained by model with these params: {np.around(third_model.best_score_, 2)}')

third_model_val_pred = third_model.predict(X_val)
print(f'3rd model - training metrics:\n\n{classification_report(y_train, third_model_train_pred)}')
print('_____________________________________________________\n')
print(f'3rd model - validation metrics:\n\n{classification_report(y_val, third_model_val_pred)}')

#training with GridSearchCV took about an hour, so we're going to train the last models with predefined numbers of estimators and maximal depth
#which should take much less time of training

fourth_model = RandomForestClassifier(
    n_estimators=1500,
    max_depth=30,
    #min_samples_split and min_samples_leaf - default values
    n_jobs=-1
)

start = time.time()
fourth_model.fit(X_train, y_train)
fourth_model_train_pred = fourth_model.predict(X_train)
stop = time.time()
mins = math.floor((stop - start)/60)
seconds = math.ceil((stop - start)%60)
print(f"Training time of RandomForestClassifier with 1500 estimators and max_depth = 30: {mins}:{seconds} mins.")

fourth_model_val_pred = fourth_model.predict(X_val)
print(f'4th model - training metrics:\n\n{classification_report(y_train, fourth_model_train_pred)}')
print('_____________________________________________________\n')
print(f'4th model - validation metrics:\n\n{classification_report(y_val, fourth_model_val_pred)}')

#RFC model with 1500 estimators and max_depth = 30 shows symptoms of overfitting 
#and returns results similar to model with default parameters with much bigger computing requirements
#so let's try the model with less estimators, but considering deeper trees

fifth_model = RandomForestClassifier(
    n_estimators=1200,
    max_depth=40,
    #min_samples_split and min_samples_leaf - default values
    n_jobs=-1
)

start = time.time()
fifth_model.fit(X_train, y_train)
fifth_model_train_pred = fifth_model.predict(X_train)
stop = time.time()
mins = math.floor((stop - start)/60)
seconds = math.ceil((stop - start)%60)
print(f"Training time of RandomForestClassifier with 1200 estimators and max_depth = 40: {mins}:{seconds} mins.")

fifth_model_val_pred = fifth_model.predict(X_val)
print(f'5th model - training metrics:\n\n{classification_report(y_train, fifth_model_train_pred)}')
print('_____________________________________________________\n')
print(f'5th model - validation metrics:\n\n{classification_report(y_val, fifth_model_val_pred)}')

#the last model will take every of nine features into consideration by setting 'None' as value of parameter max_features

sixth_model = RandomForestClassifier(max_features=None)

start = time.time()
sixth_model.fit(X_train, y_train)
sixth_model_train_pred = sixth_model.predict(X_train)
stop = time.time()
mins = math.floor((stop - start)/60)
seconds = math.ceil((stop - start)%60)
print(f"Training time of RandomForestClassifier taking every feature into consideration: {mins}:{seconds} mins.")

sixth_model_val_pred = sixth_model.predict(X_val)
print(f'6th model - training metrics:\n\n{classification_report(y_train, sixth_model_train_pred)}')
print('_____________________________________________________\n')
print(f'6th model - validation metrics:\n\n{classification_report(y_val, sixth_model_val_pred)}')

#X_selected = df[['price', 'vehicleType', 'yearOfRegistration',
        #'powerPS', 'gearbox']]